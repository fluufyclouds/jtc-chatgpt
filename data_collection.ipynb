{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in ./new-env/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in ./new-env/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: fake_useragent in ./new-env/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in ./new-env/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in ./new-env/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy in ./new-env/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./new-env/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./new-env/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./new-env/lib/python3.11/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./new-env/lib/python3.11/site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./new-env/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./new-env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./new-env/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./new-env/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in ./new-env/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in ./new-env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 fake_useragent pandas openpyxl numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import urllib.parse\n",
    "import time\n",
    "\n",
    "def google_search(query, num_results, time_filter = None):\n",
    "    # initialize UserAgent\n",
    "    ua = UserAgent()\n",
    "    # generate a random user agent for each request\n",
    "    headers = {'User-Agent': ua.random}\n",
    "\n",
    "    # URL encode the query\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "\n",
    "    # construct the Google search URL\n",
    "    google_url = f\"https://www.google.com/search?q={query}&num={num_results}\"\n",
    "\n",
    "    # append the time filter if specified\n",
    "    if time_filter:\n",
    "        google_url += f\"&tbs={time_filter}\"\n",
    "\n",
    "    attempts = 0\n",
    "    while attempts < 5:\n",
    "        # send the request\n",
    "        response = requests.get(google_url, headers=headers)\n",
    "        attempts += 1\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            search_results = []\n",
    "\n",
    "            # extract search result URLs\n",
    "            for g in soup.find_all('div', class_='g'):\n",
    "                anchors = g.find_all('a')\n",
    "                if anchors:\n",
    "                    link = anchors[0]['href']\n",
    "                    search_results.append(link)\n",
    "                \n",
    "            return search_results\n",
    "\n",
    "        elif response.status_code == 429:\n",
    "            print(\"rate limit reached, waiting to retry...\")\n",
    "            time.sleep(1 * attempts)  # exponential back-off\n",
    "\n",
    "        else:\n",
    "            print(f\"failed to retrieve search results: status code {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quarters(start_year, end_year):\n",
    "    quarters = {}\n",
    "    if end_year == 2024:\n",
    "        quarters[\"2024 Q1\"] = \"cdr:1,cd_min:1/1/2024,cd_max:3/31/2024\"\n",
    "        quarters[\"2024 Q2\"] = \"cdr:1,cd_min:4/1/2024,cd_max:6/30/2024\"\n",
    "        end_year -= 1\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        quarters[f\"{year} Q1\"] = f\"cdr:1,cd_min:1/1/{year},cd_max:3/31/{year}\"\n",
    "        quarters[f\"{year} Q2\"] = f\"cdr:1,cd_min:4/1/{year},cd_max:6/30/{year}\"\n",
    "        quarters[f\"{year} Q3\"] = f\"cdr:1,cd_min:7/1/{year},cd_max:9/30/{year}\"\n",
    "        quarters[f\"{year} Q4\"] = f\"cdr:1,cd_min:10/1/{year},cd_max:12/31/{year}\"\n",
    "    return quarters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2020 - 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  URLs  Quarter\n",
      "0    https://www.colliers.com/en-sg/news/2024-02-14...  2024 Q1\n",
      "1    https://www.colliers.com/en-sg/news/2024-02-14...  2024 Q1\n",
      "2    https://www.businesstimes.com.sg/property/bt-p...  2024 Q1\n",
      "3    https://www.colliers.com/en-sg/research/2023-q...  2024 Q1\n",
      "4    https://www.jll.com.sg/en/newsroom/moderation-...  2024 Q1\n",
      "..                                                 ...      ...\n",
      "907  https://www.ebmpapst.com/sg/en/newsroom/news/2...  2023 Q4\n",
      "908  https://esr-logosreit.listedcompany.com/newsro...  2023 Q4\n",
      "909  https://www.propertyguru.com.sg/property-for-r...  2023 Q4\n",
      "910  https://www.mof.gov.sg/news-publications/speec...  2023 Q4\n",
      "911  https://www.ema.gov.sg/news-events/news/featur...  2023 Q4\n",
      "\n",
      "[912 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "quarter_dictionary = generate_quarters(2020, 2024)\n",
    "\n",
    "query = \"singapore industrial market outlook news\"\n",
    "# create a base DataFrame\n",
    "headers = [\"URLs\", \"Quarter\"]\n",
    "df = pd.DataFrame(columns=headers)\n",
    "\n",
    "# iterate over queries and quarters\n",
    "for quarter, time_filter in quarter_dictionary.items():\n",
    "    # perform Google search\n",
    "    results = google_search(query, num_results=100, time_filter=time_filter)\n",
    "    # append results to DataFrame\n",
    "    temp_df = pd.DataFrame({\"URLs\": results, \"Quarter\": quarter})\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "# output the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to an Excel File\n",
    "df.to_excel(\"web_scraping_urls.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2016 - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   URLs  Quarter\n",
      "0     https://www.todayonline.com/singapore/jtc-impr...  2015 Q1\n",
      "1     https://www.straitstimes.com/singapore/manufac...  2015 Q1\n",
      "2     https://www.todayonline.com/singapore/singapor...  2015 Q1\n",
      "3             https://thejden.sg/general-property-news/  2015 Q1\n",
      "4                              https://eurocham.org.sg/  2015 Q1\n",
      "...                                                 ...      ...\n",
      "1106  https://www.srx.com.sg/commercial/henderson-in...  2019 Q4\n",
      "1107    https://www.eurekalert.org/news-releases/500897  2019 Q4\n",
      "1108                               https://www.ifs.com/  2019 Q4\n",
      "1109  https://www.99.co/singapore/insider/99-co-to-c...  2019 Q4\n",
      "1110  https://www.propertyguru.com.sg/project/inno-c...  2019 Q4\n",
      "\n",
      "[1111 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "quarter_dictionary = generate_quarters(2015, 2019)\n",
    "\n",
    "query = \"singapore industrial market outlook news\"\n",
    "# create a base DataFrame\n",
    "headers = [\"URLs\", \"Quarter\"]\n",
    "new_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "# iterate over queries and quarters\n",
    "for quarter, time_filter in quarter_dictionary.items():\n",
    "    # perform Google search\n",
    "    results = google_search(query, num_results=100, time_filter=time_filter)\n",
    "    # append results to DataFrame\n",
    "    temp_df = pd.DataFrame({\"URLs\": results, \"Quarter\": quarter})\n",
    "    new_df = pd.concat([new_df, temp_df], ignore_index=True)\n",
    "\n",
    "# output the DataFrame\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to an existing Excel file\n",
    "file_path = '/Users/loowenwen/Desktop/Visual Code Studio/jtc-chatgpt/web_scraping_urls.xlsx'\n",
    "existing_data = pd.read_excel(file_path)\n",
    "updated_data = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# write the updated DataFrame back to the same Excel file\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    updated_data.to_excel(writer, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2010 - 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate limit reached, waiting to retry...\n",
      "rate limit reached, waiting to retry...\n",
      "rate limit reached, waiting to retry...\n",
      "rate limit reached, waiting to retry...\n",
      "rate limit reached, waiting to retry...\n",
      "rate limit reached, waiting to retry...\n",
      "rate limit reached, waiting to retry...\n",
      "rate limit reached, waiting to retry...\n",
      "rate limit reached, waiting to retry...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# iterate over queries and quarters\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m quarter, time_filter \u001b[38;5;129;01min\u001b[39;00m quarter_dictionary\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# perform Google search\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# append results to DataFrame\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURLs\u001b[39m\u001b[38;5;124m\"\u001b[39m: results, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuarter\u001b[39m\u001b[38;5;124m\"\u001b[39m: quarter})\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mgoogle_search\u001b[0;34m(query, num_results, time_filter)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate limit reached, waiting to retry...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m attempts)  \u001b[38;5;66;03m# exponential back-off\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to retrieve search results: status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "quarter_dictionary = generate_quarters(2020, 2024)\n",
    "\n",
    "query = \"singapore industrial market outlook news\"\n",
    "# create a base DataFrame\n",
    "headers = [\"URLs\", \"Quarter\"]\n",
    "df = pd.DataFrame(columns=headers)\n",
    "\n",
    "# iterate over queries and quarters\n",
    "for quarter, time_filter in quarter_dictionary.items():\n",
    "    # perform Google search\n",
    "    results = google_search(query, num_results=100, time_filter=time_filter)\n",
    "    # append results to DataFrame\n",
    "    temp_df = pd.DataFrame({\"URLs\": results, \"Quarter\": quarter})\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "# output the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to an Excel File\n",
    "df.to_excel(\"web_scraping_urls.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
